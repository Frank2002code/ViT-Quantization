{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a AMD64 machine\n",
      "Backend is fbgemm\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "machine = platform.machine()\n",
    "print(f\"This is a {machine} machine\")\n",
    "\n",
    "backend = \"\"\n",
    "if machine == 'AMD64':\n",
    "    # backend = 'x86'\n",
    "    backend = 'fbgemm'\n",
    "elif machine == 'arm64':\n",
    "    backend = 'qnnpack'\n",
    "print(f\"Backend is {backend}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Subset\n",
    "import torch.ao.quantization as Q\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "T.backends.quantized.engine = backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VisionConfig:\n",
    "    num_hidden_layers: int = 12 # number of hidden layers in the encoder as in the paper\n",
    "    num_channels: int = 3\n",
    "    embed_dim: int = 512  # patch_size * patch_size * num_channels\n",
    "    image_size: int = 32\n",
    "    patch_size: int = 4\n",
    "    num_attention_heads: int = 8  # embed_dim // 64\n",
    "    hidden_size: int = 512  # embed_dim\n",
    "    intermediate_size: int = 144  # 4 * hidden_size\n",
    "    layer_norm_eps: float = 1e-6\n",
    "    attention_dropout: float = 0.0\n",
    "    \n",
    "model_config = VisionConfig()\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    batch_size: int = 1\n",
    "    subset_size: int = 100\n",
    "    num_workers: int = 4\n",
    "    \n",
    "dataset_config = DatasetConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with float32 and int8\n",
    "transform_fp32 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(\n",
    "    #         mean=[0.485, 0.456, 0.406],\n",
    "    #         std=[0.229, 0.224, 0.225]\n",
    "    # )\n",
    "])\n",
    "def pil_to_tensor(img):\n",
    "    return T.from_numpy(np.array(img)).permute(2, 0, 1)\n",
    "transform_int8 = transforms.Compose([\n",
    "    transforms.Lambda(pil_to_tensor),    \n",
    "])\n",
    "\n",
    "# Shape: (B, C, H, W)\n",
    "full_dataset_fp32 = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_fp32,\n",
    ")\n",
    "full_dataset_int8 = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_int8,\n",
    ")\n",
    "\n",
    "# Subset for calibration\n",
    "indices = np.random.choice(\n",
    "    a=len(full_dataset_fp32),\n",
    "    size=dataset_config.subset_size,\n",
    "    replace=False\n",
    ")\n",
    "calibration_dataset_fp32 = Subset(full_dataset_fp32, indices)\n",
    "calibration_loader_fp32 = T.utils.data.DataLoader(\n",
    "    dataset=calibration_dataset_fp32,\n",
    "    batch_size=dataset_config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=dataset_config.num_workers,\n",
    ")\n",
    "\n",
    "calibration_dataset_int8 = Subset(full_dataset_int8, indices)\n",
    "calibration_loader_int8 = T.utils.data.DataLoader(\n",
    "    dataset=calibration_dataset_int8,\n",
    "    batch_size=dataset_config.batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 100\n",
      "Number of calibration batches: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of data points: {len(calibration_dataset_fp32)}\\nNumber of calibration batches: {len(calibration_loader_fp32)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32 input shape: torch.Size([1, 3, 32, 32])\n",
      "tensor([0.6353, 0.6353, 0.6549, 0.6745, 0.6980, 0.7137, 0.7216, 0.7216, 0.7059,\n",
      "        0.7176, 0.6784, 0.5412, 0.4902, 0.5569, 0.6157, 0.5490, 0.4157, 0.3882,\n",
      "        0.3843, 0.4314, 0.4667, 0.4314, 0.3412, 0.3529, 0.4118, 0.4824, 0.6078,\n",
      "        0.6667, 0.6824, 0.6863, 0.6902, 0.6588])\n",
      "Int8 input shape: torch.Size([1, 3, 32, 32])\n",
      "tensor([162, 162, 167, 172, 178, 182, 184, 184, 180, 183, 173, 138, 125, 142,\n",
      "        157, 140, 106,  99,  98, 110, 119, 110,  87,  90, 105, 123, 155, 170,\n",
      "        174, 175, 176, 168], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# float32 input for check the model\n",
    "input_tensor_fp32 = next(iter(calibration_loader_fp32))[0]\n",
    "print(f\"Float32 input shape: {input_tensor_fp32.shape}\\n{input_tensor_fp32[0, 0, 0]}\")\n",
    "\n",
    "# int8 input for quantization\n",
    "input_tensor_int8 = next(iter(calibration_loader_int8))[0]\n",
    "print(f\"Int8 input shape: {input_tensor_int8.shape}\\n{input_tensor_int8[0, 0, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub()\n",
       "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a floating point model where some layers could be statically quantized\n",
    "class M(T.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = Q.QuantStub()\n",
    "        self.conv = T.nn.Conv2d(1, 1, 1)\n",
    "        self.relu = T.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = Q.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'x86' for server inference and 'qnnpack'\n",
    "# for mobile inference. Other quantization configurations such as selecting\n",
    "# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n",
    "# can be specified here.\n",
    "# Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n",
    "# for server inference.\n",
    "# model_fp32.qconfig = Q.get_default_qconfig('fbgemm')\n",
    "model_fp32.qconfig = Q.get_default_qconfig(backend)\n",
    "\n",
    "## 運算融合\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "model_fp32_fused = Q.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = Q.prepare(model_fp32_fused)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = T.randn(4, 1, 4, 4)\n",
    "model_fp32_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = Q.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): Quantize(scale=tensor([0.0233]), zero_point=tensor([107]), dtype=torch.quint8)\n",
       "  (conv): QuantizedConvReLU2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=0.007877849042415619, zero_point=0)\n",
       "  (relu): Identity()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEmbeddings(nn.Module):\n",
    "  def __init__(self, config: VisionConfig):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "\n",
    "    self.num_channels = config.num_channels  # 3 for RGB\n",
    "    self.embed_dim = config.embed_dim  # 512\n",
    "    self.image_size = config.image_size  # 32\n",
    "    self.patch_size = config.patch_size  # 4\n",
    "\n",
    "    self.patch_embedding = nn.Conv2d(\n",
    "      in_channels=self.num_channels,\n",
    "      out_channels=self.embed_dim,\n",
    "      kernel_size=self.patch_size,\n",
    "      stride=self.patch_size,\n",
    "      padding=0,\n",
    "    )\n",
    "\n",
    "    self.num_patches = (self.image_size // self.patch_size) ** 2  # （32/4）^2 = 64\n",
    "    self.num_positions = self.num_patches\n",
    "    self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "    self.register_buffer(\n",
    "      \"position_ids\",\n",
    "      T.arange(self.num_positions).expand((1, -1)),\n",
    "      persistent=False,\n",
    "    )\n",
    "    \n",
    "    self.quant = Q.QuantStub()\n",
    "    self.dequant = Q.DeQuantStub()\n",
    "\n",
    "  def forward(self, pixel_values: T.FloatTensor) -> T.Tensor:\n",
    "    # B, C, H, W = pixel_values.shape\n",
    "    pixel_values = self.quant(pixel_values)\n",
    "    patch_embeds = self.patch_embedding(pixel_values)\n",
    "    patch_embeds = self.dequant(patch_embeds)\n",
    "    \n",
    "    embeddings = patch_embeds.flatten(start_dim=2, end_dim=-1)\n",
    "    embeddings = embeddings.transpose(1, 2)\n",
    "    embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "embd_fp32 = VisionEmbeddings(model_config).eval()\n",
    "print(f\"Shape: {embd_fp32(input_tensor_fp32).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the VisionEmbeddings module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embd_calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with T.no_grad():\n",
    "        for img, _ in data_loader:\n",
    "            model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\quant\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def quantize_model(model: nn.Module, model_name: str, calibrate_fn: callable) -> nn.Module:\n",
    "    model.qconfig = Q.get_default_qconfig(backend)\n",
    "    if model_name == 'embd':\n",
    "        model.position_embedding.qconfig = Q.float_qparams_weight_only_qconfig\n",
    "\n",
    "    # Prepare the model for static quantization. This inserts observers in\n",
    "    # the model that will observe activation tensors during calibration.\n",
    "    model = Q.prepare(model)\n",
    "    calibrate_fn(model, calibration_loader_fp32)\n",
    "    model = Q.convert(model)\n",
    "    return model\n",
    "\n",
    "embd_int8 = quantize_model(\n",
    "    model=embd_fp32,\n",
    "    model_name='embd',\n",
    "    calibrate_fn=embd_calibrate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEmbeddings(\n",
       "  (patch_embedding): QuantizedConv2d(3, 512, kernel_size=(4, 4), stride=(4, 4), scale=0.02523711696267128, zero_point=62)\n",
       "  (position_embedding): QuantizedEmbedding(num_embeddings=64, embedding_dim=512, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
       "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_fp32 = None\n",
    "def save_data(\n",
    "    data_loader: T.utils.data.DataLoader,\n",
    "    model: nn.Module,\n",
    "    model_name: str = \"embd\",\n",
    "    dir_path: str = \"result/input\",\n",
    ") -> None:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    scale = model.quant.scale\n",
    "    zero_point = model.quant.zero_point\n",
    "    if scale is not None:\n",
    "        scale = scale.detach().numpy()\n",
    "        index =  math.ceil(math.log2(0.5/scale.item()))\n",
    "        scale_file_path = os.path.join(dir_path, \"scale.npy\")\n",
    "        index_file_path = os.path.join(dir_path, \"index.npy\")\n",
    "        np.save(scale_file_path, scale)\n",
    "        np.save(index_file_path, index)\n",
    "    if zero_point is not None:\n",
    "        zero_point = zero_point.detach().numpy()\n",
    "        zero_point_file_path = os.path.join(dir_path, \"zero_point.npy\")\n",
    "        np.save(zero_point_file_path, zero_point)\n",
    "    datas_int8 = []\n",
    "    if model_name == \"embd\":\n",
    "        for img_fp32, _ in data_loader:\n",
    "            data_fp32 = model(img_fp32)\n",
    "            data_int8 = (T.round(data_fp32 / scale) + zero_point).to(T.int8)\n",
    "            datas_int8.append(data_int8.numpy())\n",
    "    elif model_name == \"attn\":\n",
    "        for img_fp32, _ in data_loader:\n",
    "            data_fp32 = attn_fp32(embd_int8(img_fp32)).detach()\n",
    "            data_int8 = (T.round(data_fp32 / scale) + zero_point).to(T.int8)\n",
    "            datas_int8.append(data_int8.numpy())\n",
    "    datas_int8_np = np.stack(datas_int8, axis=0)\n",
    "    print(f\"Shape: {datas_int8_np.shape}\")\n",
    "    if model_name == \"embd\":\n",
    "        data_file_path = os.path.join(dir_path, \"input.npy\")\n",
    "    elif model_name == \"attn\":\n",
    "        data_file_path = os.path.join(dir_path, \"qk.npy\")\n",
    "    np.save(data_file_path, datas_int8_np)\n",
    "    \n",
    "    print(f\"Saved input data to {dir_path}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (100, 1, 64, 512)\n",
      "Saved input data to result/input/\n"
     ]
    }
   ],
   "source": [
    "save_data(\n",
    "    data_loader=calibration_loader_fp32,\n",
    "    model=embd_int8,\n",
    "    model_name=\"embd\",\n",
    "    dir_path=\"result/input\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.int8'>, Shpae: (100, 1, 64, 512)\n",
      "[ 113  106   71  -85 -121   84   81  113 -127  100   84 -127  126   81\n",
      "   97  -98 -111  103  122 -118  100  103 -108  -92 -124   78  122   87\n",
      "   90 -127  110 -121 -108  116 -108  119  116 -121   81 -118 -127  -89\n",
      "   97 -127  100  106  106 -121  -95  119 -118  126  116  -98 -111  126\n",
      "  122   74  -85   74 -118 -127 -127 -124  122 -102  122   90  -95 -121\n",
      "  106  119   84   87 -102 -105   81   94   97   81   97 -121  100   94\n",
      "  -95  -85   78  110  -98  -98  -95 -127 -124   90  122  122   81  -92\n",
      " -121   94]\n"
     ]
    }
   ],
   "source": [
    "# Check input type and shape\n",
    "input_data = np.load(\"result/input/input.npy\")\n",
    "print(f\"Type: {type(input_data[0, 0, 0, 0])}, Shpae: {input_data.shape}\\n{input_data[:, 0, 0, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSA/Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "        self.quant = Q.QuantStub()\n",
    "        self.dequant = Q.DeQuantStub()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # the hidden states are the embeddings of the patches, so (batch_size, num_patches, embed_dim)\n",
    "        B, T, E = hidden_states.shape\n",
    "        hidden_states = self.quant(hidden_states)  # int8\n",
    "        q_states = self.q_proj(hidden_states)\n",
    "        k_states = self.k_proj(hidden_states)\n",
    "        v_states = self.v_proj(hidden_states)\n",
    "        \n",
    "        np.save(\"result/output/q.npy\", q_states.detach().numpy())\n",
    "        np.save(\"result/output/k.npy\", k_states.detach().numpy())\n",
    "        np.save(\"result/output/v.npy\", v_states.detach().numpy())\n",
    "        \n",
    "        q_states = q_states.view(B, T, self.num_heads, E // self.num_heads).transpose(1, 2)\n",
    "        k_states = k_states.view(B, T, self.num_heads, E // self.num_heads).transpose(1, 2)\n",
    "        v_states = v_states.view(B, T, self.num_heads, E // self.num_heads).transpose(1, 2)\n",
    "        \n",
    "        # int8 quantization\n",
    "        q_states = self.quant(q_states)\n",
    "        k_states = self.quant(k_states)\n",
    "        v_states = self.quant(v_states)\n",
    "\n",
    "        attn_weights = (q_states @ k_states.transpose(-2, -1)) * (1.0 / math.sqrt(k_states.size(-1)))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dequant(attn_weights)  # float32\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        \n",
    "        attn_outs = attn_weights @ v_states\n",
    "        \n",
    "        attn_outs = self.quant(attn_outs)  # int8\n",
    "        attn_outs = attn_outs.transpose(1, 2)\n",
    "        attn_outs = attn_outs.reshape(B, T, E).contiguous()\n",
    "        attn_outs = self.out_proj(attn_outs)\n",
    "        attn_outs = self.dequant(attn_outs)\n",
    "        return attn_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionQK(nn.Module):\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "                \n",
    "        self.quant = Q.QuantStub()\n",
    "        self.dequant = Q.DeQuantStub()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # the hidden states are the embeddings of the patches, so (batch_size, num_patches, embed_dim)\n",
    "        B, T, E = hidden_states.shape\n",
    "        hidden_states = self.quant(hidden_states)  # int8\n",
    "        q_states = self.q_proj(hidden_states)\n",
    "        k_states = self.k_proj(hidden_states)\n",
    "        v_states = self.v_proj(hidden_states)\n",
    "        \n",
    "        os.makedirs(\"result/output/Q\", exist_ok=True)\n",
    "        os.makedirs(\"result/output/K\", exist_ok=True)\n",
    "        os.makedirs(\"result/output/V\", exist_ok=True)\n",
    "        np.save(\"result/output/Q/q_fp32.npy\", q_states.detach().numpy())\n",
    "        np.save(\"result/output/K/k_fp32.npy\", k_states.detach().numpy())\n",
    "        np.save(\"result/output/V/v_fp32.npy\", v_states.detach().numpy())\n",
    "\n",
    "        q_states = q_states.view(B, T, self.num_heads, E // self.num_heads).transpose(1, 2)\n",
    "        k_states = k_states.view(B, T, self.num_heads, E // self.num_heads).transpose(1, 2)\n",
    "        v_states = v_states.view(B, T, self.num_heads, E // self.num_heads).transpose(1, 2)\n",
    "        \n",
    "        # int8 quantization\n",
    "        q_states = self.quant(q_states)\n",
    "        k_states = self.quant(k_states)\n",
    "        v_states = self.quant(v_states)\n",
    "\n",
    "        QK = (q_states @ k_states.transpose(-2, -1)) * (1.0 / math.sqrt(k_states.size(-1)))\n",
    "        QK = self.quant(QK)\n",
    "        \n",
    "        return QK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 8, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "attn_fp32 = AttentionQK(model_config).eval()\n",
    "QK = attn_fp32(embd_int8(input_tensor_fp32))\n",
    "print(f\"Shape: {QK.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with T.no_grad():\n",
    "        for img, _ in data_loader:\n",
    "            model(embd_int8(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\quant\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AttentionQK(\n",
       "  (k_proj): QuantizedLinear(in_features=512, out_features=512, scale=0.03887570649385452, zero_point=62, qscheme=torch.per_channel_affine)\n",
       "  (v_proj): QuantizedLinear(in_features=512, out_features=512, scale=0.03652804717421532, zero_point=66, qscheme=torch.per_channel_affine)\n",
       "  (q_proj): QuantizedLinear(in_features=512, out_features=512, scale=0.03661142289638519, zero_point=62, qscheme=torch.per_channel_affine)\n",
       "  (quant): Quantize(scale=tensor([0.0617]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_int8 = quantize_model(attn_fp32, 'attn', attn_calibrate)\n",
    "attn_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Weight Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weight(\n",
    "    model: nn.Module,\n",
    "    model_name: str = \"attn\",\n",
    "    dir_path: str = \"result/weights\",\n",
    "):\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    def extract_weight_bias(qlinear: T.nn.quantized.Linear) -> tuple[T.Tensor, T.Tensor]:\n",
    "        \"\"\"Extract packed weight, bias, scale, zero_point from quantized Linear layer\"\"\"\n",
    "        weight, bias = qlinear.weight(), qlinear.bias()\n",
    "        # scale = qlinear.scale\n",
    "        # zero_point = qlinear.zero_point\n",
    "        return weight, bias\n",
    "\n",
    "    for layer_name, param in model.state_dict().items():\n",
    "        if isinstance(param, T.Tensor):\n",
    "            weight_name = layer_name.split('.')[0]\n",
    "            if \"scale\" in layer_name:\n",
    "                scale_file_path = os.path.join(dir_path, f\"{weight_name}_scale.npy\")\n",
    "                index_file_path = os.path.join(dir_path, f\"{weight_name}_index.npy\")\n",
    "                index =  math.ceil(math.log2(0.5/param.item()))\n",
    "                np.save(scale_file_path, param.detach().numpy())\n",
    "                np.save(index_file_path, index)\n",
    "                print(f\"✅Saved {weight_name} scale\")\n",
    "            elif \"zero_point\" in layer_name:\n",
    "                file_path = os.path.join(dir_path, f\"{weight_name}_zero_point.npy\")\n",
    "                np.save(file_path, param.detach().numpy())\n",
    "                print(f\"✅Saved {weight_name} zero_point\")\n",
    "        elif isinstance(param, tuple):\n",
    "            weight_file_path = os.path.join(dir_path, f\"{weight_name}_weight.npy\")\n",
    "            bias_file_path = os.path.join(dir_path, f\"{weight_name}_bias_fp32.npy\")\n",
    "            layer = getattr(model, weight_name)\n",
    "            weight, bias = extract_weight_bias(layer)\n",
    "            np.save(weight_file_path, weight.detach().int_repr().numpy())\n",
    "            np.save(bias_file_path, bias.detach().numpy())\n",
    "            print(f\"✅Saved {weight_name} weight and bias\")\n",
    "        else:\n",
    "            print(f\"⚠️Skip {layer_name} (Not Tensor, type: {type(param)})\")\n",
    "        print(\"----------------------------------\")\n",
    "\n",
    "    print(f\"{model_name} weights have been saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅Saved k_proj scale\n",
      "----------------------------------\n",
      "✅Saved k_proj zero_point\n",
      "----------------------------------\n",
      "⚠️Skip k_proj._packed_params.dtype (Not Tensor, type: <class 'torch.dtype'>)\n",
      "----------------------------------\n",
      "✅Saved k_proj weight and bias\n",
      "----------------------------------\n",
      "✅Saved v_proj scale\n",
      "----------------------------------\n",
      "✅Saved v_proj zero_point\n",
      "----------------------------------\n",
      "⚠️Skip v_proj._packed_params.dtype (Not Tensor, type: <class 'torch.dtype'>)\n",
      "----------------------------------\n",
      "✅Saved v_proj weight and bias\n",
      "----------------------------------\n",
      "✅Saved q_proj scale\n",
      "----------------------------------\n",
      "✅Saved q_proj zero_point\n",
      "----------------------------------\n",
      "⚠️Skip q_proj._packed_params.dtype (Not Tensor, type: <class 'torch.dtype'>)\n",
      "----------------------------------\n",
      "✅Saved q_proj weight and bias\n",
      "----------------------------------\n",
      "✅Saved quant scale\n",
      "----------------------------------\n",
      "✅Saved quant zero_point\n",
      "----------------------------------\n",
      "attn weights have been saved!\n"
     ]
    }
   ],
   "source": [
    "save_weight(\n",
    "    model=attn_int8,\n",
    "    model_name=\"attn\",\n",
    "    dir_path=\"result/weights\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_to_int8(\n",
    "    dir_path: str = \"result/weights\",\n",
    "    weight_name: str = \"k_proj\",\n",
    "):\n",
    "    bias_path = os.path.join(dir_path, f\"{weight_name}_bias_fp32.npy\")\n",
    "    scale = np.load(os.path.join(dir_path, f\"{weight_name}_scale.npy\"))\n",
    "    zero_point = np.load(os.path.join(dir_path, f\"{weight_name}_zero_point.npy\"))\n",
    "    bias_fp32 = np.load(bias_path)\n",
    "    bias_int8 = (T.round(T.tensor(bias_fp32) / scale) + zero_point).to(T.int8)\n",
    "    bias_int8_file_path = bias_path.replace(\"bias\", \"bias_int8\")\n",
    "    np.save(bias_int8_file_path, bias_int8)\n",
    "    print(f\"Saved int8 bias to {bias_int8_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved int8 bias to result/weights\\k_proj_bias_int8_fp32.npy\n",
      "Saved int8 bias to result/weights\\v_proj_bias_int8_fp32.npy\n",
      "Saved int8 bias to result/weights\\q_proj_bias_int8_fp32.npy\n"
     ]
    }
   ],
   "source": [
    "bias_to_int8(weight_name=\"k_proj\")\n",
    "bias_to_int8(weight_name=\"v_proj\")\n",
    "bias_to_int8(weight_name=\"q_proj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.int8'>, Shape: (512, 512)\n",
      "-82\n",
      "Type: <class 'numpy.int8'>, Shape: (512,)\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "# Check weight type and shape\n",
    "weight_data = np.load(\"result/weights/k_proj_weight.npy\")\n",
    "print(f\"Type: {type(weight_data[0, 0])}, Shape: {weight_data.shape}\\n{weight_data[0, 0]}\")\n",
    "\n",
    "bias_data = np.load(\"result/weights/k_proj_bias_int8.npy\")\n",
    "print(f\"Type: {type(bias_data[0])}, Shape: {bias_data.shape}\\n{bias_data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_int8(\n",
    "    dir_path: str = \"result/output/Q\",\n",
    "    output_name: str = \"k\",\n",
    "):\n",
    "    output_path = os.path.join(dir_path, f\"{output_name}_fp32.npy\")\n",
    "    output = np.load(output_path)\n",
    "    scale = np.load(f\"result/weights/{output_name}_proj_scale.npy\")\n",
    "    zero_point = np.load(f\"result/weights/{output_name}_proj_zero_point.npy\")\n",
    "    output_int8 = (T.round(T.tensor(output) / scale) + zero_point).to(T.int8)\n",
    "    output_int8_file_path = os.path.join(dir_path, f\"{output_name}_int8.npy\")\n",
    "    np.save(output_int8_file_path, output_int8)\n",
    "    print(f\"Saved int8 bias to {output_int8_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved int8 bias to result/output/Q\\q_int8.npy\n",
      "Saved int8 bias to result/output/K\\k_int8.npy\n",
      "Saved int8 bias to result/output/V\\v_int8.npy\n"
     ]
    }
   ],
   "source": [
    "output_to_int8(\n",
    "    dir_path=\"result/output/Q\",\n",
    "    output_name=\"q\",\n",
    ")\n",
    "output_to_int8(\n",
    "    dir_path=\"result/output/K\",\n",
    "    output_name=\"k\",\n",
    ")\n",
    "output_to_int8(\n",
    "    dir_path=\"result/output/V\",\n",
    "    output_name=\"v\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (100, 1, 8, 64, 64)\n",
      "Saved input data to result/output/QK/\n"
     ]
    }
   ],
   "source": [
    "save_data(\n",
    "    data_loader=calibration_loader_fp32,\n",
    "    model=attn_int8,\n",
    "    model_name=\"attn\",\n",
    "    dir_path=\"result/output/QK\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP: MLP(\n",
      "  (fc1): Linear(in_features=48, out_features=144, bias=True)\n",
      "  (fc2): Linear(in_features=144, out_features=48, bias=True)\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "Shape: torch.Size([1, 64, 48])\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.quant = Q.QuantStub()\n",
    "        self.dequant = Q.DeQuantStub()\n",
    "\n",
    "    def forward(self, hidden_states: T.Tensor) -> T.Tensor:\n",
    "        hidden_states = self.quant(hidden_states)\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = nn.functional.gelu(hidden_states, approximate=\"tanh\")\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = self.dequant(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "mlp = MLP(model_config)\n",
    "print(f\"MLP: {mlp}\\nShape: {mlp(embd_fp32(input_tensor_fp32[:1])).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = Attention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = MLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        \n",
    "        # quantization\n",
    "        self.quant = T.ao.quantization.QuantStub()\n",
    "        self.dequant = T.ao.quantization.DeQuantStub()\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.quant(hidden_states)\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states = self.self_attn(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.dequant(hidden_states)\n",
    "\n",
    "        hidden_states = self.quant(hidden_states)\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.dequant(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "encoder_layer_32 = EncoderLayer(VisionConfig(hidden_size=768, intermediate_size=3072))\n",
    "encoder_layer_32(T.randn(1, 196, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderLayer(\n",
       "  (self_attn): Attention(\n",
       "    (k_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.017260996624827385, zero_point=130, qscheme=torch.per_tensor_affine)\n",
       "    (v_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.01868443191051483, zero_point=128, qscheme=torch.per_tensor_affine)\n",
       "    (q_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.017791934311389923, zero_point=125, qscheme=torch.per_tensor_affine)\n",
       "    (out_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.0009516198770143092, zero_point=124, qscheme=torch.per_tensor_affine)\n",
       "    (quant): Quantize(scale=tensor([0.0284]), zero_point=tensor([127]), dtype=torch.quint8)\n",
       "    (dequant): DeQuantize()\n",
       "  )\n",
       "  (layer_norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (mlp): MLP(\n",
       "    (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.017856968566775322, zero_point=128, qscheme=torch.per_tensor_affine)\n",
       "    (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.006086959037929773, zero_point=129, qscheme=torch.per_tensor_affine)\n",
       "  )\n",
       "  (layer_norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (quant): Quantize(scale=tensor([0.0330]), zero_point=tensor([126]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer_32.qconfig = T.ao.quantization.get_default_qconfig(backend)\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "encoder_layer_32_prepared = T.ao.quantization.prepare(encoder_layer_32)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = T.randn(1, 196, 768)\n",
    "encoder_layer_32_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "encoder_layer_int8 = T.ao.quantization.convert(encoder_layer_32_prepared)\n",
    "encoder_layer_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
