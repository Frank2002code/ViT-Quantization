{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a AMD64 machine\n",
      "Backend is x86\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "machine = platform.machine()\n",
    "print(f\"This is a {machine} machine\")\n",
    "\n",
    "backend = 'fbgemm'\n",
    "if machine == 'AMD64':\n",
    "    backend = 'x86'\n",
    "elif machine == 'arm64':\n",
    "    backend = 'qnnpack'\n",
    "print(f\"Backend is {backend}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Subset\n",
    "import torch.ao.quantization as Q\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "T.backends.quantized.engine = backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VisionConfig:\n",
    "    num_hidden_layers: int = 12 # number of hidden layers in the encoder as in the paper\n",
    "    num_channels: int = 3\n",
    "    embed_dim: int = 512  # patch_size * patch_size * num_channels\n",
    "    image_size: int = 32\n",
    "    patch_size: int = 4\n",
    "    num_attention_heads: int = 3  # embed_dim // 64\n",
    "    hidden_size: int = 48\n",
    "    intermediate_size: int = 144  # 4 * hidden_size\n",
    "    layer_norm_eps: float = 1e-6\n",
    "    attention_dropout: float = 0.0\n",
    "    \n",
    "model_config = VisionConfig()\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    batch_size: int = 1\n",
    "    subset_size: int = 100\n",
    "    num_workers: int = 4\n",
    "    \n",
    "dataset_config = DatasetConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with float32 and int8\n",
    "transform_fp32 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(\n",
    "    #         mean=[0.485, 0.456, 0.406],\n",
    "    #         std=[0.229, 0.224, 0.225]\n",
    "    # )\n",
    "])\n",
    "def pil_to_tensor(img):\n",
    "    return T.from_numpy(np.array(img)).permute(2, 0, 1)\n",
    "transform_int8 = transforms.Compose([\n",
    "    transforms.Lambda(pil_to_tensor),    \n",
    "])\n",
    "\n",
    "# Shape: (B, C, H, W)\n",
    "full_dataset_fp32 = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_fp32,\n",
    ")\n",
    "full_dataset_int8 = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_int8,\n",
    ")\n",
    "\n",
    "# Subset for calibration\n",
    "indices = np.random.choice(\n",
    "    a=len(full_dataset_fp32),\n",
    "    size=dataset_config.subset_size,\n",
    "    replace=False\n",
    ")\n",
    "calibration_dataset_fp32 = Subset(full_dataset_fp32, indices)\n",
    "calibration_loader_fp32 = T.utils.data.DataLoader(\n",
    "    dataset=calibration_dataset_fp32,\n",
    "    batch_size=dataset_config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=dataset_config.num_workers,\n",
    ")\n",
    "\n",
    "calibration_dataset_int8 = Subset(full_dataset_int8, indices)\n",
    "calibration_loader_int8 = T.utils.data.DataLoader(\n",
    "    dataset=calibration_dataset_int8,\n",
    "    batch_size=dataset_config.batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 100\n",
      "Number of calibration batches: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of data points: {len(calibration_dataset_fp32)}\\nNumber of calibration batches: {len(calibration_loader_fp32)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32 input shape: torch.Size([1, 3, 32, 32])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039,\n",
      "        0.0039, 0.0039, 0.0039, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n",
      "        0.0039, 0.0039, 0.0039, 0.0078, 0.0196, 0.0275, 0.0235, 0.0196, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0196])\n",
      "Int8 input shape: torch.Size([1, 3, 32, 32])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 2, 5, 7,\n",
      "        6, 5, 4, 4, 4, 4, 4, 5], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# float32 input for check the model\n",
    "input_tensor_fp32 = next(iter(calibration_loader_fp32))[0]\n",
    "print(f\"Float32 input shape: {input_tensor_fp32.shape}\\n{input_tensor_fp32[0, 0, 0]}\")\n",
    "\n",
    "# int8 input for quantization\n",
    "input_tensor_int8 = next(iter(calibration_loader_int8))[0]\n",
    "print(f\"Int8 input shape: {input_tensor_int8.shape}\\n{input_tensor_int8[0, 0, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub()\n",
       "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a floating point model where some layers could be statically quantized\n",
    "class M(T.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = Q.QuantStub()\n",
    "        self.conv = T.nn.Conv2d(1, 1, 1)\n",
    "        self.relu = T.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = Q.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'x86' for server inference and 'qnnpack'\n",
    "# for mobile inference. Other quantization configurations such as selecting\n",
    "# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n",
    "# can be specified here.\n",
    "# Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n",
    "# for server inference.\n",
    "# model_fp32.qconfig = Q.get_default_qconfig('fbgemm')\n",
    "model_fp32.qconfig = Q.get_default_qconfig(backend)\n",
    "\n",
    "## 運算融合\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "model_fp32_fused = Q.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = Q.prepare(model_fp32_fused)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = T.randn(4, 1, 4, 4)\n",
    "model_fp32_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = Q.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): Quantize(scale=tensor([0.0233]), zero_point=tensor([107]), dtype=torch.quint8)\n",
       "  (conv): QuantizedConvReLU2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=0.007877849042415619, zero_point=0)\n",
       "  (relu): Identity()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEmbeddings(nn.Module):\n",
    "  def __init__(self, config: VisionConfig):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "\n",
    "    self.num_channels = config.num_channels  # 3 for RGB\n",
    "    self.embed_dim = config.embed_dim  # 512\n",
    "    self.image_size = config.image_size  # 32\n",
    "    self.patch_size = config.patch_size  # 4\n",
    "\n",
    "    self.patch_embedding = nn.Conv2d(\n",
    "      in_channels=self.num_channels,\n",
    "      out_channels=self.embed_dim,\n",
    "      kernel_size=self.patch_size,\n",
    "      stride=self.patch_size,\n",
    "      padding=0,\n",
    "    )\n",
    "\n",
    "    self.num_patches = (self.image_size // self.patch_size) ** 2  # （32/4）^2 = 64\n",
    "    self.num_positions = self.num_patches\n",
    "    self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "    self.register_buffer(\n",
    "      \"position_ids\",\n",
    "      T.arange(self.num_positions).expand((1, -1)),\n",
    "      persistent=False,\n",
    "    )\n",
    "    \n",
    "    self.quant = Q.QuantStub()\n",
    "    self.dequant = Q.DeQuantStub()\n",
    "\n",
    "  def forward(self, pixel_values: T.FloatTensor) -> T.Tensor:\n",
    "    # B, C, H, W = pixel_values.shape\n",
    "    pixel_values = self.quant(pixel_values)\n",
    "    patch_embeds = self.patch_embedding(pixel_values)\n",
    "    patch_embeds = self.dequant(patch_embeds)\n",
    "    \n",
    "    embeddings = patch_embeds.flatten(start_dim=2, end_dim=-1)\n",
    "    embeddings = embeddings.transpose(1, 2)\n",
    "    embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "embd_fp32 = VisionEmbeddings(model_config).eval()\n",
    "print(f\"Shape: {embd_fp32(input_tensor_fp32).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the VisionEmbeddings module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embd_calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with T.no_grad():\n",
    "        for img, _ in data_loader:\n",
    "            model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model: nn.Module, model_name: str, calibrate_fn: callable) -> nn.Module:\n",
    "    model.qconfig = Q.get_default_qconfig(backend)\n",
    "    if model_name == 'embd':\n",
    "        model.position_embedding.qconfig = Q.float_qparams_weight_only_qconfig\n",
    "\n",
    "    # Prepare the model for static quantization. This inserts observers in\n",
    "    # the model that will observe activation tensors during calibration.\n",
    "    model = Q.prepare(model)\n",
    "    calibrate_fn(model, calibration_loader_fp32)\n",
    "    model = Q.convert(model)\n",
    "    return model\n",
    "\n",
    "embd_int8 = quantize_model(\n",
    "    model=embd_fp32,\n",
    "    model_name='embd',\n",
    "    calibrate_fn=embd_calibrate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEmbeddings(\n",
       "  (patch_embedding): QuantizedConv2d(3, 512, kernel_size=(4, 4), stride=(4, 4), scale=0.02934972196817398, zero_point=69)\n",
       "  (position_embedding): QuantizedEmbedding(num_embeddings=64, embedding_dim=512, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
       "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Type: torch.int8\n",
      "Shape: (100, 1, 64, 512)\n",
      "Saved input data to result/input/\n"
     ]
    }
   ],
   "source": [
    "def save_data(\n",
    "    data_loader: T.utils.data.DataLoader,\n",
    "    model: nn.Module,\n",
    "    dir_path: str = \"result/input\",\n",
    ") -> None:\n",
    "    scale = model.quant.scale\n",
    "    zero_point = model.quant.zero_point\n",
    "    if scale is not None:\n",
    "        scale = scale.detach().numpy()\n",
    "        index =  math.ceil(math.log2(0.5/scale.item()))\n",
    "        scale_file_path = os.path.join(dir_path, \"scale.npy\")\n",
    "        index_file_path = os.path.join(dir_path, \"index.npy\")\n",
    "        np.save(scale_file_path, scale)\n",
    "        np.save(index_file_path, index)\n",
    "    if zero_point is not None:\n",
    "        zero_point = zero_point.detach().numpy()\n",
    "        zero_point_file_path = os.path.join(dir_path, \"zero_point.npy\")\n",
    "        np.save(zero_point_file_path, zero_point)\n",
    "    datas_int8 = []\n",
    "    for img_fp32, idx in data_loader:\n",
    "        data_fp32 = embd_int8(img_fp32)\n",
    "        data_int8 = (T.round(data_fp32 / scale) + zero_point).to(T.int8)\n",
    "        datas_int8.append(data_int8.numpy())\n",
    "    datas_int8_np = np.stack(datas_int8, axis=0)\n",
    "    print(f\"Shape: {datas_int8_np.shape}\")\n",
    "    data_file_path = os.path.join(dir_path, \"input.npy\")\n",
    "    np.save(data_file_path, datas_int8_np)\n",
    "    \n",
    "    print(f\"Saved input data to {dir_path}/\")\n",
    "\n",
    "save_data(\n",
    "    data_loader=calibration_loader_fp32,\n",
    "    model=embd_int8,\n",
    "    dir_path=\"result/input\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.int8'>, Shpae: (100, 1, 64, 512)\n",
      "[-120  -67  -98  -84  -95  -88  -67 -105  -77 -127  -81 -116 -120  -67\n",
      " -127  -74  -67  -67  -88  -67  -67 -123  -81 -109  -95  -81 -123 -102\n",
      "  -81 -102  -95 -105  -98  -88  -67  -88  -70  -95 -123 -123  -74  -95\n",
      "  -91 -105  -67 -105 -123  119  -84  -70 -127  -77 -109  -74  -81  -74\n",
      " -102 -116 -120 -127 -113  -98  -77  -81 -120  -67  -95  -95  -98 -113\n",
      " -116  -74 -105  -95 -116  -77 -123 -123  -95  -88  -81 -102  -98  -91\n",
      " -116  -77  -74  -67 -113  -70 -105  -98 -127  -88 -113 -109  -88  -70\n",
      "  -95  -95]\n"
     ]
    }
   ],
   "source": [
    "# Check input type and shape\n",
    "input_data = np.load(\"result/input/input.npy\")\n",
    "print(f\"Type: {type(input_data[0, 0, 0, 0])}, Shpae: {input_data.shape}\\n{input_data[:, 0, 0, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSA/Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "        self.quant = Q.QuantStub()\n",
    "        self.dequant = Q.DeQuantStub()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # the hidden states are the embeddings of the patches, so (batch_size, num_patches, embed_dim)\n",
    "        B, T, E = hidden_states.shape\n",
    "        hidden_states = self.quant(hidden_states)  # int8\n",
    "        q_states = self.q_proj(hidden_states)\n",
    "        k_states = self.k_proj(hidden_states)\n",
    "        v_states = self.v_proj(hidden_states)\n",
    "\n",
    "        q_states = q_states.view(B, T, self.num_heads, E // self.num_heads).transpose(1, 2)\n",
    "        k_states = k_states.view(B, T, self.num_heads, E // self.num_heads).transpose(1, 2)\n",
    "        v_states = v_states.view(B, T, self.num_heads, E // self.num_heads).transpose(1, 2)\n",
    "        \n",
    "        # int8 quantization\n",
    "        q_states = self.quant(q_states)\n",
    "        k_states = self.quant(k_states)\n",
    "        v_states = self.quant(v_states)\n",
    "\n",
    "        attn_weights = (q_states @ k_states.transpose(-2, -1)) * (1.0 / math.sqrt(k_states.size(-1)))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dequant(attn_weights)  # float32\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        \n",
    "        attn_outs = attn_weights @ v_states\n",
    "        \n",
    "        attn_outs = self.quant(attn_outs)  # int8\n",
    "        attn_outs = attn_outs.transpose(1, 2)\n",
    "        attn_outs = attn_outs.reshape(B, T, E).contiguous()\n",
    "        attn_outs = self.out_proj(attn_outs)\n",
    "        attn_outs = self.dequant(attn_outs)\n",
    "        return attn_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (k_proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "  (v_proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "  (q_proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "  (out_proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_32 = Attention(model_config).eval()\n",
    "attn_32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with T.no_grad():\n",
    "        for img, _ in data_loader:\n",
    "            model(embd_int8(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yydsyyds/anaconda3/envs/seq2seq/lib/python3.12/site-packages/torch/ao/quantization/observer.py:1318: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (k_proj): QuantizedLinear(in_features=48, out_features=48, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "  (v_proj): QuantizedLinear(in_features=48, out_features=48, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "  (q_proj): QuantizedLinear(in_features=48, out_features=48, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "  (out_proj): QuantizedLinear(in_features=48, out_features=48, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_int8 = quantize_model(attn_32, 'attn', attn_calibrate)\n",
    "attn_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.5777374 , -0.20340034, -0.26059115, ..., -0.1801702 ,\n",
       "           0.05257939, -0.37605557],\n",
       "         [ 0.07773671, -0.4927413 , -0.5331277 , ..., -0.17008369,\n",
       "          -0.1610846 ,  0.2325479 ],\n",
       "         [-1.2353468 , -0.5868912 , -0.8142619 , ...,  0.5414332 ,\n",
       "          -0.3233808 , -0.12766021],\n",
       "         ...,\n",
       "         [-0.4429888 , -0.03145947, -0.5798223 , ..., -0.11446146,\n",
       "          -0.10229073,  0.15403038],\n",
       "         [-0.5578185 ,  0.10655621,  0.31388047, ...,  0.40862387,\n",
       "          -0.10129814,  0.2565011 ],\n",
       "         [ 0.08744194, -0.04625409, -0.15002586, ..., -0.10700875,\n",
       "          -0.2133356 ,  0.03119151]],\n",
       "\n",
       "        [[-0.6749521 ,  0.07763428, -0.20227295, ...,  0.13347055,\n",
       "           0.37727988,  0.09413266],\n",
       "         [ 0.4384782 ,  0.18808739,  0.41153464, ...,  0.00200756,\n",
       "          -0.05175502,  0.18237492],\n",
       "         [ 0.03340231, -0.31025946,  0.48925775, ..., -0.10289848,\n",
       "          -0.19903986, -0.2742836 ],\n",
       "         ...,\n",
       "         [-0.46215126, -0.1157022 ,  0.27943885, ..., -0.4734129 ,\n",
       "          -0.41291553,  0.14241308],\n",
       "         [-0.36363077, -0.08285498, -0.5937588 , ...,  0.24891013,\n",
       "          -0.01749287,  0.0409239 ],\n",
       "         [-0.19710597,  0.03117809, -0.6496294 , ...,  0.05767564,\n",
       "          -0.34767786, -0.3043466 ]],\n",
       "\n",
       "        [[-0.08466914, -0.1784332 , -0.2445967 , ..., -0.70561004,\n",
       "          -0.12122136, -0.4667457 ],\n",
       "         [ 1.7529061 , -0.13012473, -0.5336521 , ...,  0.34444687,\n",
       "          -0.10477983, -1.0250915 ],\n",
       "         [ 0.58956957,  0.2128962 ,  0.03711785, ..., -0.1753387 ,\n",
       "          -0.38072795, -1.3902296 ],\n",
       "         ...,\n",
       "         [-0.24629629,  0.13972525,  0.7435976 , ..., -0.38062108,\n",
       "          -0.01547696,  0.18928112],\n",
       "         [-0.20108835,  0.24631274,  0.3817328 , ...,  0.44966042,\n",
       "          -0.062955  ,  0.71057826],\n",
       "         [ 0.53289574, -0.02265273,  0.13270031, ...,  0.4116025 ,\n",
       "           0.2949669 ,  0.07319254]]],\n",
       "\n",
       "\n",
       "       [[[-0.5257321 , -0.47784406, -0.25169632, ..., -0.15896048,\n",
       "           0.28727382, -0.0034303 ],\n",
       "         [ 0.2311185 , -0.13609436, -0.20165107, ..., -0.36303475,\n",
       "          -0.27624878,  0.15475349],\n",
       "         [-0.9108786 ,  0.16792639, -0.36480016, ...,  0.5161999 ,\n",
       "          -0.71760833, -0.42319527],\n",
       "         ...,\n",
       "         [-0.2666012 ,  0.391762  , -0.5041692 , ...,  0.03784436,\n",
       "          -0.37972406, -0.0774032 ],\n",
       "         [ 0.14228065,  0.22416747, -0.04861648, ...,  0.30170497,\n",
       "          -0.47217512,  0.10734747],\n",
       "         [ 0.09655329, -0.3715439 , -0.5004466 , ..., -0.10565571,\n",
       "          -0.3261934 ,  0.15669622]],\n",
       "\n",
       "        [[-0.54642165,  0.263688  , -0.19569609, ..., -0.10969067,\n",
       "           0.1886152 , -0.18437323],\n",
       "         [ 0.58045167,  0.44666433,  0.24516787, ...,  0.11664645,\n",
       "          -0.55748314, -0.22619136],\n",
       "         [-0.13734679, -0.39150906,  0.57350355, ..., -0.11358911,\n",
       "          -0.36185116, -0.5812676 ],\n",
       "         ...,\n",
       "         [-0.50288826, -0.25480253,  0.3770617 , ..., -0.36041757,\n",
       "           0.07156095,  0.70878536],\n",
       "         [-0.46876836, -0.10839871, -0.26981127, ..., -0.10933224,\n",
       "           0.09073929,  0.25704318],\n",
       "         [-0.12066042,  0.1392845 , -0.64589876, ..., -0.09391622,\n",
       "          -0.25539818, -0.44059613]],\n",
       "\n",
       "        [[-0.07426797,  0.08850067, -0.00789553, ..., -0.28222823,\n",
       "          -0.23705144, -0.2682474 ],\n",
       "         [ 1.2482399 , -0.33655262, -0.48345566, ...,  0.40342218,\n",
       "          -0.05454048, -0.2732459 ],\n",
       "         [ 0.3629711 ,  0.1314881 , -0.01519541, ...,  0.17737354,\n",
       "          -0.11198279, -1.0285733 ],\n",
       "         ...,\n",
       "         [-0.59327745, -0.05858882,  0.6711064 , ..., -0.0219829 ,\n",
       "          -0.15499675, -0.3175208 ],\n",
       "         [ 0.05642435,  0.21354344,  0.46068892, ..., -0.02977662,\n",
       "          -0.36770085, -0.08472402],\n",
       "         [ 1.1735988 , -0.2935001 , -0.15513422, ...,  0.4005411 ,\n",
       "           0.54669595, -0.37701932]]],\n",
       "\n",
       "\n",
       "       [[[-0.4758913 , -0.39742893, -0.3540601 , ..., -0.38948944,\n",
       "           0.17821532, -0.28485593],\n",
       "         [ 0.19333325, -0.2867517 , -0.5252245 , ..., -0.1999838 ,\n",
       "          -0.07193486,  0.27969417],\n",
       "         [-0.23709217,  0.41994005, -0.04677976, ...,  1.1928047 ,\n",
       "           0.41429055,  0.56967294],\n",
       "         ...,\n",
       "         [-0.03221527,  0.31691888, -0.4772113 , ..., -0.04327708,\n",
       "           0.02720383,  0.27745408],\n",
       "         [ 0.04595577,  0.62402314,  0.6171027 , ...,  0.55735594,\n",
       "           0.2581333 ,  0.36010075],\n",
       "         [ 0.22436814,  0.1602708 , -0.29032177, ..., -0.00722813,\n",
       "          -0.15659115,  0.05646642]],\n",
       "\n",
       "        [[-0.41805547,  0.31556976, -0.48972744, ...,  0.53056026,\n",
       "           0.5293001 ,  0.10922074],\n",
       "         [ 0.724472  ,  0.38067338,  0.0267551 , ...,  0.10172635,\n",
       "          -0.47659147, -0.04775432],\n",
       "         [-0.24250534, -0.7050558 , -0.39151755, ..., -0.27923736,\n",
       "          -0.82960826, -0.7702875 ],\n",
       "         ...,\n",
       "         [-0.56685376, -0.2077743 , -0.28531817, ..., -0.31824675,\n",
       "          -0.6098545 , -0.0421629 ],\n",
       "         [-0.23085743, -0.0054958 , -0.93547744, ...,  0.40562317,\n",
       "          -0.15645444, -0.0992892 ],\n",
       "         [ 0.04037473,  0.17737642, -0.8953619 , ...,  0.35258958,\n",
       "          -0.31271204, -0.26759386]],\n",
       "\n",
       "        [[ 0.15560846,  0.00418342,  0.11190862, ...,  0.08931035,\n",
       "           0.18585671,  0.14381601],\n",
       "         [ 1.573002  , -0.30027536, -0.411468  , ...,  0.8765794 ,\n",
       "          -0.07458898, -0.70130736],\n",
       "         [ 0.2805931 , -0.12437229,  0.01925836, ...,  0.36399493,\n",
       "          -0.30113348, -0.98162365],\n",
       "         ...,\n",
       "         [-0.61099285, -0.163262  ,  0.83580244, ..., -0.20745337,\n",
       "          -0.22032869,  0.25340453],\n",
       "         [-0.44067243,  0.15771896,  0.74295455, ...,  0.47398576,\n",
       "          -0.42804694,  0.67584574],\n",
       "         [ 0.33983496, -0.28085127,  0.29753077, ...,  0.6816634 ,\n",
       "           0.14917362,  0.3284452 ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[-0.3882208 , -0.38449582, -0.03358699, ..., -0.11274128,\n",
       "           0.31045645, -0.10677592],\n",
       "         [-0.05861652, -0.6670327 , -0.70870644, ..., -0.12051809,\n",
       "          -0.5188647 ,  0.05865835],\n",
       "         [-1.195712  , -0.8835398 , -1.08875   , ...,  0.6135324 ,\n",
       "          -0.73880625, -0.51249194],\n",
       "         ...,\n",
       "         [-0.5458836 , -0.16588195, -0.7478351 , ..., -0.12634432,\n",
       "          -0.35932258, -0.00770433],\n",
       "         [-0.44617304, -0.34090582, -0.16640268, ...,  0.44348088,\n",
       "          -0.42865938, -0.01611277],\n",
       "         [ 0.06917038, -0.4310776 , -0.709736  , ...,  0.1889741 ,\n",
       "          -0.3590613 ,  0.00257661]],\n",
       "\n",
       "        [[-0.955794  , -0.08864373, -0.16980678, ...,  0.03351371,\n",
       "           0.3330544 , -0.37087995],\n",
       "         [ 0.31902978,  0.11155537,  0.3566695 , ..., -0.3614812 ,\n",
       "          -0.7444341 , -0.28068516],\n",
       "         [-0.08645175, -0.36529753,  0.5758536 , ..., -0.40724656,\n",
       "          -0.46299544, -0.52054375],\n",
       "         ...,\n",
       "         [-0.48903862, -0.01893734,  0.25679544, ..., -0.40786213,\n",
       "          -0.22609319,  0.401247  ],\n",
       "         [-0.4492909 , -0.0634308 , -0.40151125, ...,  0.02592351,\n",
       "           0.21623868,  0.03263718],\n",
       "         [-0.19534877, -0.02373165, -0.6250065 , ..., -0.12456471,\n",
       "          -0.2727803 , -0.33044365]],\n",
       "\n",
       "        [[ 0.11936986, -0.2549646 , -0.5026206 , ..., -0.7133    ,\n",
       "          -0.3186714 , -1.1548724 ],\n",
       "         [ 1.8227651 , -0.23569418, -0.6995482 , ...,  0.30101487,\n",
       "          -0.2511169 , -1.4554977 ],\n",
       "         [ 0.90255195,  0.16194496, -0.06975269, ..., -0.18163471,\n",
       "          -0.3252279 , -1.8629156 ],\n",
       "         ...,\n",
       "         [-0.07585705,  0.12651064,  0.814605  , ..., -0.39879814,\n",
       "          -0.04622399,  0.02069339],\n",
       "         [ 0.43867442,  0.18239225,  0.3041831 , ...,  0.11184023,\n",
       "          -0.40616554, -0.35238248],\n",
       "         [ 1.4096332 , -0.20211351, -0.02603182, ...,  0.3553357 ,\n",
       "           0.4195329 , -0.8411604 ]]],\n",
       "\n",
       "\n",
       "       [[[-0.62320316, -0.64001566, -0.44440857, ..., -0.01495116,\n",
       "           0.1355308 , -0.10392847],\n",
       "         [-0.0969695 ,  0.01628672, -0.15392417, ..., -0.17960727,\n",
       "          -0.13031068,  0.22052652],\n",
       "         [-1.29775   , -0.28954795, -0.22045428, ..., -0.10875707,\n",
       "          -0.5405937 , -0.2701259 ],\n",
       "         ...,\n",
       "         [-0.38154584, -0.04229282, -0.7225772 , ..., -0.12559319,\n",
       "          -0.20923693,  0.14398576],\n",
       "         [-0.58713317,  0.10720247,  0.08454909, ...,  0.22830483,\n",
       "          -0.27837932, -0.01450295],\n",
       "         [ 0.14048038, -0.22764017, -0.4127141 , ...,  0.14468287,\n",
       "          -0.16471468,  0.11757318]],\n",
       "\n",
       "        [[-0.75740224,  0.09129263, -0.1292186 , ..., -0.07510533,\n",
       "           0.38735583, -0.08885968],\n",
       "         [ 0.6191384 ,  0.7122205 ,  0.8536153 , ..., -0.08800468,\n",
       "          -0.57564396,  0.08575104],\n",
       "         [ 0.1102626 , -0.27324504,  0.32676354, ...,  0.07031001,\n",
       "          -0.32549307, -0.44313902],\n",
       "         ...,\n",
       "         [-0.39954424, -0.24647959,  0.02114708, ..., -0.24433647,\n",
       "          -0.04774826,  0.32419783],\n",
       "         [-0.3027688 , -0.14098445, -0.7180921 , ...,  0.27825537,\n",
       "           0.08182863, -0.01408337],\n",
       "         [-0.12263128, -0.05249958, -0.91988087, ...,  0.03691445,\n",
       "          -0.25404125, -0.39099002]],\n",
       "\n",
       "        [[ 0.02343193, -0.14595504, -0.50853014, ..., -0.9584937 ,\n",
       "          -0.09829996, -0.8576397 ],\n",
       "         [ 1.3319712 ,  0.11506253, -0.4864056 , ...,  0.7279314 ,\n",
       "           0.18741567, -0.44527432],\n",
       "         [ 0.40139192,  0.38871506, -0.13826357, ...,  0.3336751 ,\n",
       "          -0.11770547, -0.5331846 ],\n",
       "         ...,\n",
       "         [ 0.14371231,  0.14311063,  0.42314577, ..., -0.34876442,\n",
       "           0.14693512, -0.14225727],\n",
       "         [ 0.07370158,  0.678004  ,  0.78614193, ...,  0.3032503 ,\n",
       "          -0.18551244,  0.25579026],\n",
       "         [ 0.96015537, -0.18708558, -0.18532158, ...,  0.5144628 ,\n",
       "           0.3792374 , -0.24905154]]],\n",
       "\n",
       "\n",
       "       [[[-0.47145864, -0.21908328, -0.13710468, ..., -0.16557123,\n",
       "           0.2150123 ,  0.02475504],\n",
       "         [-0.19261433,  0.10651003,  0.06323964, ..., -0.2141205 ,\n",
       "          -0.18150628, -0.01127544],\n",
       "         [-0.7551569 ,  0.8049177 , -0.10592248, ...,  0.17739893,\n",
       "          -0.47351643, -0.46526843],\n",
       "         ...,\n",
       "         [-0.3434368 , -0.27654335, -0.9817774 , ..., -0.18528657,\n",
       "          -0.26872265,  0.02220496],\n",
       "         [-0.35386407,  0.2933613 ,  0.26428127, ...,  0.2383031 ,\n",
       "          -0.26242915, -0.00635986],\n",
       "         [ 0.0959592 , -0.35085306, -0.24858078, ..., -0.1530107 ,\n",
       "          -0.17140704,  0.16085571]],\n",
       "\n",
       "        [[-0.6437938 ,  0.05984126, -0.14804915, ...,  0.18512312,\n",
       "           0.32194144, -0.30060208],\n",
       "         [ 0.64549387,  0.28750443,  0.24251966, ...,  0.11411407,\n",
       "          -0.5144719 , -0.06486417],\n",
       "         [ 0.16958773, -0.56103545,  0.17869686, ...,  0.27849957,\n",
       "          -0.41817802, -0.05710027],\n",
       "         ...,\n",
       "         [-0.62533677, -0.12840384,  0.29702878, ..., -0.41022682,\n",
       "          -0.14899902,  0.20837112],\n",
       "         [-0.37266243, -0.20437641, -0.66638416, ...,  0.26020306,\n",
       "           0.06214045,  0.11882222],\n",
       "         [-0.23452397,  0.1663778 , -0.46137798, ..., -0.09436709,\n",
       "          -0.16646937, -0.5709351 ]],\n",
       "\n",
       "        [[-0.03028586,  0.29717666,  0.4785674 , ..., -0.6839137 ,\n",
       "          -0.29373047, -0.5188347 ],\n",
       "         [ 0.8345465 , -0.45386192, -0.297512  , ...,  0.7586659 ,\n",
       "          -0.260189  , -0.631152  ],\n",
       "         [-0.24882883,  0.3554992 ,  0.55682564, ...,  0.06632387,\n",
       "          -0.3103341 , -0.65240794],\n",
       "         ...,\n",
       "         [ 0.23513445,  0.286699  ,  0.87775576, ..., -0.5564946 ,\n",
       "           0.00997302, -0.36701635],\n",
       "         [-0.05644539,  0.01908498,  0.36922932, ...,  0.21562627,\n",
       "          -0.24981855,  0.1965141 ],\n",
       "         [ 0.9860389 , -0.12821154, -0.12323068, ...,  0.51779675,\n",
       "           0.29794592, -0.4604142 ]]]],\n",
       "      shape=(16, 3, 64, 64), dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QK_output = np.load('QK_output.npy')\n",
    "QK_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Quantized MSA Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (k_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.017669403925538063, zero_point=130, qscheme=torch.per_tensor_affine)\n",
       "  (v_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.01814001053571701, zero_point=134, qscheme=torch.per_tensor_affine)\n",
       "  (q_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.018048712983727455, zero_point=131, qscheme=torch.per_tensor_affine)\n",
       "  (out_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.0008625031914561987, zero_point=131, qscheme=torch.per_tensor_affine)\n",
       "  (quant): Quantize(scale=tensor([0.0274]), zero_point=tensor([127]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP: MLP(\n",
      "  (fc1): Linear(in_features=48, out_features=144, bias=True)\n",
      "  (fc2): Linear(in_features=144, out_features=48, bias=True)\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "Shape: torch.Size([1, 64, 48])\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.quant = Q.QuantStub()\n",
    "        self.dequant = Q.DeQuantStub()\n",
    "\n",
    "    def forward(self, hidden_states: T.Tensor) -> T.Tensor:\n",
    "        hidden_states = self.quant(hidden_states)\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = nn.functional.gelu(hidden_states, approximate=\"tanh\")\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = self.dequant(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "mlp = MLP(model_config)\n",
    "print(f\"MLP: {mlp}\\nShape: {mlp(embd_fp32(input_tensor_fp32[:1])).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = Attention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = MLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        \n",
    "        # quantization\n",
    "        self.quant = T.ao.quantization.QuantStub()\n",
    "        self.dequant = T.ao.quantization.DeQuantStub()\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.quant(hidden_states)\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states = self.self_attn(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.dequant(hidden_states)\n",
    "\n",
    "        hidden_states = self.quant(hidden_states)\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.dequant(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "encoder_layer_32 = EncoderLayer(VisionConfig(hidden_size=768, intermediate_size=3072))\n",
    "encoder_layer_32(T.randn(1, 196, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderLayer(\n",
       "  (self_attn): Attention(\n",
       "    (k_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.017260996624827385, zero_point=130, qscheme=torch.per_tensor_affine)\n",
       "    (v_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.01868443191051483, zero_point=128, qscheme=torch.per_tensor_affine)\n",
       "    (q_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.017791934311389923, zero_point=125, qscheme=torch.per_tensor_affine)\n",
       "    (out_proj): QuantizedLinear(in_features=768, out_features=768, scale=0.0009516198770143092, zero_point=124, qscheme=torch.per_tensor_affine)\n",
       "    (quant): Quantize(scale=tensor([0.0284]), zero_point=tensor([127]), dtype=torch.quint8)\n",
       "    (dequant): DeQuantize()\n",
       "  )\n",
       "  (layer_norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (mlp): MLP(\n",
       "    (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.017856968566775322, zero_point=128, qscheme=torch.per_tensor_affine)\n",
       "    (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.006086959037929773, zero_point=129, qscheme=torch.per_tensor_affine)\n",
       "  )\n",
       "  (layer_norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (quant): Quantize(scale=tensor([0.0330]), zero_point=tensor([126]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer_32.qconfig = T.ao.quantization.get_default_qconfig(backend)\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "encoder_layer_32_prepared = T.ao.quantization.prepare(encoder_layer_32)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = T.randn(1, 196, 768)\n",
    "encoder_layer_32_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "encoder_layer_int8 = T.ao.quantization.convert(encoder_layer_32_prepared)\n",
    "encoder_layer_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: nn.Module, model_name: str):\n",
    "    save_dir = f\"{model_name}_quant_weights\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    def extract_packed_params(qlinear):\n",
    "        \"\"\"Extract packed weight, bias, scale, zero_point from quantized Linear layer\"\"\"\n",
    "        packed_weight, bias = qlinear.weight(), qlinear.bias()\n",
    "        scale = qlinear.scale\n",
    "        zero_point = qlinear.zero_point\n",
    "        return packed_weight, bias, scale, zero_point\n",
    "\n",
    "    for name, param in model.state_dict().items():\n",
    "        if isinstance(param, T.Tensor):  # 只存 Tensor，避免 dtype 錯誤\n",
    "            print(f\"{name}: {param.shape}\")\n",
    "            layer_name = name.split(\".\")[0]\n",
    "            \n",
    "            # 處理量化 Linear 層，提取 `_packed_params`\n",
    "            layer = getattr(model, layer_name)\n",
    "            if isinstance(layer, T.nn.quantized.Linear):\n",
    "                if hasattr(model, layer_name):\n",
    "                    packed_weight, bias, scale, zero_point = extract_packed_params(layer)\n",
    "                    # print(f\"{packed_weight}, {bias}, {scale}, {zero_point}\")\n",
    "\n",
    "                    # 存儲 `int8` 權重\n",
    "                    int8_weight = packed_weight.int_repr().cpu().numpy()\n",
    "                    np.save(os.path.join(save_dir, f\"{name}.npy\"), int8_weight)\n",
    "                    print(f\"✅ Saved {layer_name}.weight as int8 (scale={scale}, zero_point={zero_point})\")\n",
    "\n",
    "                    # 存儲 `float32` bias\n",
    "                    np.save(os.path.join(save_dir, f\"{name}_bias.npy\"), bias.detach().numpy())\n",
    "                    print(f\"✅ Saved {layer_name}.bias as float32\")\n",
    "            # 如果是量化張量，先轉為 int8\n",
    "            elif param.dtype in [T.qint8, T.quint8]:\n",
    "                # print(param)\n",
    "                zero_point = 0\n",
    "                if param.qscheme == T.per_tensor_affine:\n",
    "                    zero_point = param.q_zero_point()\n",
    "                    int8_tensor = (param.int_repr() - zero_point).cpu().numpy()  # 轉成 int8\n",
    "                else:\n",
    "                    int8_tensor = param.int_repr().cpu().numpy()\n",
    "                np.save(os.path.join(save_dir, f\"{name}.npy\"), int8_tensor)  # 存成 .npy\n",
    "                print(f\"✅ Saved {name} as int8 (zero_point={zero_point})\")\n",
    "            else:\n",
    "                weight_arr = param.detach().numpy()\n",
    "                np.save(os.path.join(save_dir, f\"{name}.npy\"), weight_arr)\n",
    "                print(f\"✅ Saved {name} as float32\")\n",
    "        else:\n",
    "            print(f\"⚠️ Skip {name} (Not Tensor, type: {type(param)})\")\n",
    "\n",
    "    print(f\" {model_name} weights have been saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embedding.weight: torch.Size([48, 3, 4, 4])\n",
      "✅ Saved patch_embedding.weight as int8 (zero_point=0)\n",
      "patch_embedding.bias: torch.Size([48])\n",
      "✅ Saved patch_embedding.bias as float32\n",
      "patch_embedding.scale: torch.Size([])\n",
      "✅ Saved patch_embedding.scale as float32\n",
      "patch_embedding.zero_point: torch.Size([])\n",
      "✅ Saved patch_embedding.zero_point as float32\n",
      "⚠️ Skip position_embedding._packed_params.dtype (Not Tensor, type: <class 'torch.dtype'>)\n",
      "position_embedding._packed_params._packed_weight: torch.Size([64, 48])\n",
      "✅ Saved position_embedding._packed_params._packed_weight as int8 (zero_point=0)\n",
      "quant.scale: torch.Size([1])\n",
      "✅ Saved quant.scale as float32\n",
      "quant.zero_point: torch.Size([1])\n",
      "✅ Saved quant.zero_point as float32\n",
      " Embeddings weights have been saved!\n"
     ]
    }
   ],
   "source": [
    "save_model(embd_int8, 'Embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_proj.scale: torch.Size([])\n",
      "✅ Saved k_proj.weight as int8 (scale=0.02344927377998829, zero_point=124)\n",
      "✅ Saved k_proj.bias as float32\n",
      "k_proj.zero_point: torch.Size([])\n",
      "✅ Saved k_proj.weight as int8 (scale=0.02344927377998829, zero_point=124)\n",
      "✅ Saved k_proj.bias as float32\n",
      "⚠️ Skip k_proj._packed_params.dtype (Not Tensor, type: <class 'torch.dtype'>)\n",
      "⚠️ Skip k_proj._packed_params._packed_params (Not Tensor, type: <class 'tuple'>)\n",
      "v_proj.scale: torch.Size([])\n",
      "✅ Saved v_proj.weight as int8 (scale=0.022955600172281265, zero_point=131)\n",
      "✅ Saved v_proj.bias as float32\n",
      "v_proj.zero_point: torch.Size([])\n",
      "✅ Saved v_proj.weight as int8 (scale=0.022955600172281265, zero_point=131)\n",
      "✅ Saved v_proj.bias as float32\n",
      "⚠️ Skip v_proj._packed_params.dtype (Not Tensor, type: <class 'torch.dtype'>)\n",
      "⚠️ Skip v_proj._packed_params._packed_params (Not Tensor, type: <class 'tuple'>)\n",
      "q_proj.scale: torch.Size([])\n",
      "✅ Saved q_proj.weight as int8 (scale=0.020705705508589745, zero_point=123)\n",
      "✅ Saved q_proj.bias as float32\n",
      "q_proj.zero_point: torch.Size([])\n",
      "✅ Saved q_proj.weight as int8 (scale=0.020705705508589745, zero_point=123)\n",
      "✅ Saved q_proj.bias as float32\n",
      "⚠️ Skip q_proj._packed_params.dtype (Not Tensor, type: <class 'torch.dtype'>)\n",
      "⚠️ Skip q_proj._packed_params._packed_params (Not Tensor, type: <class 'tuple'>)\n",
      "out_proj.scale: torch.Size([])\n",
      "✅ Saved out_proj.weight as int8 (scale=0.006773641798645258, zero_point=125)\n",
      "✅ Saved out_proj.bias as float32\n",
      "out_proj.zero_point: torch.Size([])\n",
      "✅ Saved out_proj.weight as int8 (scale=0.006773641798645258, zero_point=125)\n",
      "✅ Saved out_proj.bias as float32\n",
      "⚠️ Skip out_proj._packed_params.dtype (Not Tensor, type: <class 'torch.dtype'>)\n",
      "⚠️ Skip out_proj._packed_params._packed_params (Not Tensor, type: <class 'tuple'>)\n",
      "quant.scale: torch.Size([1])\n",
      "✅ Saved quant.scale as float32\n",
      "quant.zero_point: torch.Size([1])\n",
      "✅ Saved quant.zero_point as float32\n",
      " Attention weights have been saved!\n"
     ]
    }
   ],
   "source": [
    "save_model(attn_int8, 'Attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
